{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset:https://www.figure-eight.com/data-for-everyone/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains labels for the emotional content (such as happiness, sadness, and anger) of texts.\n",
    "Hundreds to thousands of examples across 13 labels.Data is coolected from twitter.Our goal is to explore data and create a \n",
    "model to predict emotion presented in given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 4 columns):\n",
      "tweet_id     40000 non-null int64\n",
      "sentiment    40000 non-null object\n",
      "author       40000 non-null object\n",
      "content      40000 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       8638\n",
       "worry         8459\n",
       "happiness     5209\n",
       "sadness       5165\n",
       "love          3842\n",
       "surprise      2187\n",
       "fun           1776\n",
       "relief        1526\n",
       "hate          1323\n",
       "empty          827\n",
       "enthusiasm     759\n",
       "boredom        179\n",
       "anger          110\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data:Remove punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.array(dataset.content)\n",
    "labels = np.array(dataset.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(features):\n",
    "    \n",
    "    clean_features = []\n",
    "    for feature in features:\n",
    "        clean_feature = []\n",
    "        # Replace multiple dots with space\n",
    "        feature = re.sub('\\.\\.+', ' ', feature)\n",
    "        # Remove single dots\n",
    "        feature = re.sub('\\.', '', feature)\n",
    "        #feature = re.sub('[^ a-zA-Z0-9]', '', feature)\n",
    "        for word in feature.split():\n",
    "            if not any(x in word for x in punctuation):\n",
    "                clean_feature.append(word)\n",
    "        clean_features.append(clean_feature)\n",
    "    return clean_features         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(features,labels):\n",
    "    features = clean_data(features)\n",
    "    '''create string of all combined features'''\n",
    "    complete_text = []\n",
    "    for feature in features:\n",
    "        for word in feature:\n",
    "            complete_text.append(word)\n",
    "    '''Create vocab and assign number to every word'''      \n",
    "    from collections import Counter\n",
    "    counts = Counter(complete_text)\n",
    "    print('counts Length:{}'.format(len(counts)))\n",
    "    vocab = sorted(counts,key=counts.get,reverse=True)\n",
    "    print('voacb Length:{}'.format(len(vocab)))\n",
    "    '''Start indexing from 1 and not 0 because later we will pad with 0'''\n",
    "    vocab_int = {word:i for i,word in enumerate(vocab,1)}\n",
    "    print('vocab_int Length:{}'.format(len(vocab_int)))\n",
    "    '''Convert features with words to features with integers'''\n",
    "    int_features = []\n",
    "    for feature in features:\n",
    "        int_feature = []\n",
    "        for word in feature:\n",
    "            int_feature.append(vocab_int[word])\n",
    "        int_features.append(int_feature)\n",
    "        \n",
    "    '''Remove features with zero length'''\n",
    "    non_zero_idx = [ii for ii, feature in enumerate(int_features ) if len(feature) != 0]\n",
    "    int_features = [int_features[ii] for ii in non_zero_idx]\n",
    "    labels = np.array([labels[ii] for ii in non_zero_idx])\n",
    "    \n",
    "    '''Encode labels'''\n",
    "    labels = pd.get_dummies(labels)\n",
    "    '''save preprocessed data'''\n",
    "    \n",
    "    pickle.dump((int_features,labels,vocab_int),open('preprocess.p', 'wb'))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts Length:33746\n",
      "voacb Length:33746\n",
      "vocab_int Length:33746\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_save_data(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_features,labels,vocab_int = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data:Decide length of feature vecor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_lenghts = []\n",
    "for feature in int_features:\n",
    "    feat_lenghts.append(len(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6179.,  7302.,  6762.,  5647.,  4678.,  4028.,  2931.,  1613.,\n",
       "          509.,    73.]),\n",
       " array([  1. ,   4.1,   7.2,  10.3,  13.4,  16.5,  19.6,  22.7,  25.8,\n",
       "         28.9,  32. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b6e8c7f6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(x=feat_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We will keep size of input vector to 25.For inputs with size less than 25,left pad 0 and for greater than 25,ignore remaining values'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We will keep size of input vector to 25.For inputs with size less than 25,left pad 0 and for greater than 25,ignore remaining values'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 25\n",
    "features = np.zeros((len(int_features), seq_len), dtype=int)\n",
    "for i, row in enumerate(int_features):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    7,   48,\n",
       "           7,   22, 3908,    1,  106, 4916,  891,    6,    7,  577, 1404,\n",
       "          23,  161,  519],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0, 9061,  303,  142,   19,    4,  444, 4917, 3075,\n",
       "          14,   37,  271]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>boredom</th>\n",
       "      <th>empty</th>\n",
       "      <th>enthusiasm</th>\n",
       "      <th>fun</th>\n",
       "      <th>happiness</th>\n",
       "      <th>hate</th>\n",
       "      <th>love</th>\n",
       "      <th>neutral</th>\n",
       "      <th>relief</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>worry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   anger  boredom  empty  enthusiasm  fun  happiness  hate  love  neutral  \\\n",
       "0      0        0      1           0    0          0     0     0        0   \n",
       "1      0        0      0           0    0          0     0     0        0   \n",
       "\n",
       "   relief  sadness  surprise  worry  \n",
       "0       0        0         0      0  \n",
       "1       0        1         0      0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39722"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of features in training set:(29791, 25)\n",
      "No of features in validation set:(4965, 25)\n",
      "No of features in test set:(4966, 25)\n",
      "No of labels in training set:(29791, 13)\n",
      "No of labels in validation set:(4965, 13)\n",
      "No of labels in test set:(4966, 13)\n"
     ]
    }
   ],
   "source": [
    "split_frac_train = 0.75\n",
    "training_ids = int(len(features)*split_frac_train)\n",
    "x_train,x_val = features[:training_ids], features[training_ids:]\n",
    "y_train,y_val = labels[:training_ids], labels[training_ids:]\n",
    "\n",
    "split_frac_test = 0.5\n",
    "testing_ids = int(len(x_val)*split_frac_test)\n",
    "x_val,x_test = x_val[:testing_ids], x_val[testing_ids:]\n",
    "y_val,y_test = y_val[:testing_ids], y_val[testing_ids:]\n",
    "\n",
    "print('No of features in training set:{}'.format(x_train.shape))\n",
    "print('No of features in validation set:{}'.format((x_val.shape)))\n",
    "print('No of features in test set:{}'.format((x_test.shape)))\n",
    "\n",
    "print('No of labels in training set:{}'.format((y_train.shape)))\n",
    "print('No of labels in validation set:{}'.format((y_val.shape)))\n",
    "print('No of labels in test set:{}'.format((y_test.shape)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "lstm_size = 256 #No of LSTM nodes\n",
    "lstm_layers = 1 #No of LSTM layers\n",
    "embed_size = 300 #No of embedding nodes\n",
    "batch_size = 512 #No of features to be fed to network in oe iteration\n",
    "learning_rate = 0.01 #Learning rate\n",
    "\n",
    "#Graph dimensions\n",
    "'''\n",
    "Assume:\n",
    "lstm_size = 256 #No of LSTM nodes\n",
    "lstm_layers = 1 #No of LSTM layers\n",
    "embed_size = 300 #No of embedding nodes\n",
    "batch_size = 512 #No of features to be fed to network in oe iteration\n",
    "\n",
    "Then Dimensions:\n",
    "input: 512x25\n",
    "Embd:  512x25x300\n",
    "LSTM:  512x25x256\n",
    "\n",
    "But we only care about output of last cell so LSTM:LSTM[:,-1] dim become 512x256\n",
    "Reshape LSTM output for softmax layer like\n",
    "lstm output : (512)x256\n",
    "weight:        256x13\n",
    "final output:  (512)x13\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(shape=[None,None],dtype=tf.int32,name='inputs')\n",
    "targets = tf.placeholder(shape=[None,None],dtype=tf.int32,name='targets')\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = len(vocab_int)+1\n",
    "embedding = tf.Variable(tf.random_uniform((n_words,embed_size),-1,1))\n",
    "embed = tf.nn.embedding_lookup(embedding,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33747"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(300)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_size)\n",
    "drop = tf.contrib.rnn.DropoutWrapper(cell=lstm, output_keep_prob=keep_prob)\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells=[drop]*lstm_layers)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs,final_state = tf.nn.dynamic_rnn(cell=cell,inputs=embed,initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(512), Dimension(None), Dimension(256)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 13\n",
    "grad_clip=5\n",
    "\n",
    "seq_output = tf.concat(outputs[:,-1],axis=1)\n",
    "x = tf.reshape(seq_output, [-1, lstm_size])\n",
    "# Connect the RNN outputs to a softmax layer\n",
    "with tf.variable_scope('softmax'):\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "\n",
    "logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "out = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "\n",
    "# One-hot encode targets and reshape to match logits\n",
    "#y_one_hot = tf.one_hot(labels, num_classes)\n",
    "y_reshaped = tf.reshape(targets, logits.get_shape())\n",
    "\n",
    "# Softmax cross entropy loss\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Optimizer for training, using gradient clipping to control exploding gradients\n",
    "'''tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer = train_op.apply_gradients(zip(grads, tvars))'''\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# Gradient Clipping\n",
    "gradients = train_op.compute_gradients(loss)\n",
    "capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "optimizer = train_op.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x,y,batch_size):\n",
    "    no_batches = len(x)//batch_size\n",
    "    x = x[:no_batches*batch_size]\n",
    "    y = y[:no_batches*batch_size]\n",
    "    for i in range(0,len(x),batch_size):\n",
    "        yield x[i:i+batch_size],y[i:i+batch_size]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/5 Iteration: 10 Train loss: 2.040\n",
      "Epoch: 0/5 Iteration: 20 Train loss: 1.989\n",
      "Epoch: 0/5 Iteration: 30 Train loss: 1.961\n",
      "Epoch: 0/5 Iteration: 40 Train loss: 2.595\n",
      "Epoch: 0/5 Iteration: 50 Train loss: 2.092\n",
      "Val loss: 2.047\n",
      "Epoch: 1/5 Iteration: 60 Train loss: 2.317\n",
      "Epoch: 1/5 Iteration: 70 Train loss: 1.896\n",
      "Epoch: 1/5 Iteration: 80 Train loss: 1.831\n",
      "Epoch: 1/5 Iteration: 90 Train loss: 1.855\n",
      "Epoch: 1/5 Iteration: 100 Train loss: 2.140\n",
      "Val loss: 1.927\n",
      "Epoch: 1/5 Iteration: 110 Train loss: 1.938\n",
      "Epoch: 2/5 Iteration: 120 Train loss: 1.951\n",
      "Epoch: 2/5 Iteration: 130 Train loss: 1.714\n",
      "Epoch: 2/5 Iteration: 140 Train loss: 1.557\n",
      "Epoch: 2/5 Iteration: 150 Train loss: 1.467\n",
      "Val loss: 1.604\n",
      "Epoch: 2/5 Iteration: 160 Train loss: 1.656\n",
      "Epoch: 2/5 Iteration: 170 Train loss: 1.550\n",
      "Epoch: 3/5 Iteration: 180 Train loss: 1.607\n",
      "Epoch: 3/5 Iteration: 190 Train loss: 1.389\n",
      "Epoch: 3/5 Iteration: 200 Train loss: 1.144\n",
      "Val loss: 1.201\n",
      "Epoch: 3/5 Iteration: 210 Train loss: 1.234\n",
      "Epoch: 3/5 Iteration: 220 Train loss: 1.287\n",
      "Epoch: 3/5 Iteration: 230 Train loss: 1.120\n",
      "Epoch: 4/5 Iteration: 240 Train loss: 1.151\n",
      "Epoch: 4/5 Iteration: 250 Train loss: 0.997\n",
      "Val loss: 0.895\n",
      "Epoch: 4/5 Iteration: 260 Train loss: 0.993\n",
      "Epoch: 4/5 Iteration: 270 Train loss: 0.827\n",
      "Epoch: 4/5 Iteration: 280 Train loss: 0.936\n",
      "Epoch: 4/5 Iteration: 290 Train loss: 0.924\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(n_epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii,(x,y) in enumerate(get_batches(x_train,y_train,batch_size)):\n",
    "            feed = {inputs: x,\n",
    "                    targets: y,\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            \n",
    "            cost,state, _ = sess.run([loss,final_state,optimizer],feed_dict=feed)\n",
    "            \n",
    "        \n",
    "            if iteration%10==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, n_epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(cost))\n",
    "            if iteration%50==0:\n",
    "                val_batch_loss = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(x_val, y_val, batch_size):\n",
    "                    feed = {inputs: x,\n",
    "                            targets: y,\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    val_loss, val_state, _ = sess.run([loss,final_state,optimizer],feed_dict=feed)\n",
    "                    val_batch_loss.append(val_loss)\n",
    "                print(\"Val loss: {:.3f}\".format(np.mean(val_batch_loss)))\n",
    "            iteration += 1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\sentiment.ckpt\n",
      "Test loss: 2.644\n"
     ]
    }
   ],
   "source": [
    "test_cost = []\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(x_test, y_test, batch_size), 1):\n",
    "        feed = {inputs: x,\n",
    "                targets: y,\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        cost,state, _ = sess.run([loss,final_state,optimizer],feed_dict=feed)\n",
    "        test_cost.append(cost)\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
